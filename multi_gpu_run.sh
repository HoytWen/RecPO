torchrun --nproc_per_node 8 --master_port=25642 sft.py \
        --model_name meta-llama/Llama-3.2-1B-Instruct  \
        --batch_size 4 \
        --gradient_accumulation_steps 8 \
        --prompt_path ./prompt/movie2.txt \
        --logging_dir log/ \
        --output_dir output/ \
        --learning_rate 1e-5 \
        --num_train_epochs 5 \
        --eval_step 0.1 \
        --report_to wandb \
        --wandb_project RecPO \
        --wandb_name SFT-multi-gpu > log/sft.log